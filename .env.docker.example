# Docker Environment Configuration for TestTeller Agent
# Copy this file to .env before running docker-compose

# =============================================================================
# LLM PROVIDER CONFIGURATION (Required: Choose one)
# =============================================================================
# Available providers: gemini, openai, claude, llama
LLM_PROVIDER=gemini

# =============================================================================
# API KEYS (Configure based on your chosen LLM provider)
# =============================================================================

# Google Gemini (required for gemini provider)
# Get your API key from: https://aistudio.google.com/
GOOGLE_API_KEY=your_gemini_api_key_here

# OpenAI (required for openai provider, also needed for claude embeddings)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Claude (required for claude provider)
# Get your API key from: https://console.anthropic.com/
# Note: Claude also requires OPENAI_API_KEY for embeddings
CLAUDE_API_KEY=your_claude_api_key_here

# GitHub Personal Access Token (optional, for private repositories)
# Get from: https://github.com/settings/tokens
GITHUB_TOKEN=your_github_token_here

# =============================================================================
# LLM MODEL CONFIGURATION (Optional - defaults provided)
# =============================================================================

# Gemini models
GEMINI_EMBEDDING_MODEL=text-embedding-004
GEMINI_GENERATION_MODEL=gemini-2.0-flash

# OpenAI models
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_GENERATION_MODEL=gpt-4o-mini

# Claude models
CLAUDE_GENERATION_MODEL=claude-3-5-haiku-20241022

# Llama/Ollama models (for local deployment)
LLAMA_EMBEDDING_MODEL=llama3.2:1b
LLAMA_GENERATION_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://ollama:11434

# =============================================================================
# APPLICATION CONFIGURATION (Optional - defaults provided)
# =============================================================================

# ChromaDB settings
DEFAULT_COLLECTION_NAME=testteller_collection

# Logging configuration
LOG_LEVEL=Error
LOG_FORMAT=json

# Document processing
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CODE_EXTENSIONS=.py,.js,.ts,.java,.go,.rs,.cpp,.hpp,.c,.h,.cs,.rb,.php

# Output configuration
OUTPUT_FILE_PATH=/app/testteller_generated_tests/testteller-testcases.pdf
# Test case output format (md, pdf, docx)
TEST_OUTPUT_FORMAT=pdf

# Test automation configuration (Docker-optimized)
AUTOMATION_OUTPUT_DIR=/app/testteller_automated_tests
AUTOMATION_LANGUAGE=python
AUTOMATION_FRAMEWORK=pytest
BASE_URL=http://web:8000
TEST_TIMEOUT=30000
AUTOMATION_ENHANCE_BY_DEFAULT=true

# Docker-specific testing endpoints
API_BASE_URL=http://api:8000/api
SELENIUM_HUB_URL=http://selenium-hub:4444/wd/hub

# API retry configuration
API_RETRY_ATTEMPTS=3
API_RETRY_WAIT_SECONDS=2

# =============================================================================
# DUAL-FEEDBACK RAG ARCHITECTURE CONFIGURATION (NEW!)
# =============================================================================

# Enable automatic storage of high-quality generated test cases
ENABLE_TEST_CASE_FEEDBACK=true

# Minimum quality score (0.0-1.0) for storing test cases in vector store
MIN_QUALITY_SCORE_FOR_STORAGE=0.7

# Maximum number of generated test cases to store in vector store
MAX_GENERATED_TESTS_TO_STORE=1000

# Days to retain generated test cases in vector store
GENERATED_TEST_RETENTION_DAYS=90

# =============================================================================
# RAG-ENHANCED AUTOMATOR AGENT CONFIGURATION (NEW!)
# =============================================================================

# Number of context documents to retrieve for automation context discovery
AUTOMATOR_NUM_CONTEXT_DOCS=5

# Default programming language for automation
AUTOMATOR_DEFAULT_LANGUAGE=python

# Default test framework for automation  
AUTOMATOR_DEFAULT_FRAMEWORK=pytest

# Enable AI enhancement for generated tests by default
AUTOMATION_ENHANCE_BY_DEFAULT=true

# =============================================================================
# DOCKER COMPOSE USAGE EXAMPLES (Updated for RAG Architecture)
# =============================================================================

# 1. SETUP: Copy this file and configure
# cp .env.docker.example .env
# Edit .env with your API keys

# 2. START SERVICES
# For Gemini (default with self-learning enabled):
# docker-compose up -d

# For OpenAI with enhanced automation:
# LLM_PROVIDER=openai docker-compose up -d

# For Claude with feedback loops:
# LLM_PROVIDER=claude docker-compose up -d

# For Llama (requires Ollama service):
# 1. Uncomment the ollama service in docker-compose.yml
# 2. Uncomment the ollama volume
# 3. Add ollama dependency to app service  
# 4. LLM_PROVIDER=llama docker-compose up -d
# 5. Install models: docker-compose exec ollama ollama pull llama3.2:3b

# 3. USAGE EXAMPLES (RAG-Enhanced Commands)
# Configure the system:
# docker-compose exec app testteller configure

# Ingest documents with enhanced parsing:
# docker-compose exec app testteller ingest-docs /app/docs/requirements.pdf -c my_collection -e

# Generate test cases (automatically stores high-quality ones):
# docker-compose exec app testteller generate "API integration tests" -c my_collection -o tests.md

# RAG-enhanced automation with context discovery:
# docker-compose exec app testteller automate tests.md -c my_collection -l python -F pytest -E

# Check collection status and feedback data:
# docker-compose exec app testteller status -c my_collection

# 4. MONITORING
# View logs: docker-compose logs app
# View ChromaDB: docker-compose logs chromadb
