"""CLI commands for TestTeller RAG-enhanced automation generation."""

import asyncio
import logging
import os
from pathlib import Path
from typing import Optional

import typer
from typing_extensions import Annotated

# Import core utilities
from ..core.utils.loader import with_progress_bar_sync
from ..core.data_ingestion.unified_document_parser import UnifiedDocumentParser, DocumentType
from ..core.constants import SUPPORTED_LANGUAGES, SUPPORTED_FRAMEWORKS
from ..core.vector_store.chromadb_manager import ChromaDBManager
from ..core.llm.llm_manager import LLMManager
from ..config import settings

# Import RAG-enhanced automation components
from .parser.markdown_parser import MarkdownTestCaseParser
from .rag_enhanced_generator import RAGEnhancedTestGenerator

logger = logging.getLogger(__name__)

# Default values
DEFAULT_COLLECTION_NAME = "test_collection"
DEFAULT_OUTPUT_DIR = "./generated_tests"
DEFAULT_LANGUAGE = "python"
DEFAULT_FRAMEWORK = "pytest"


def get_collection_name(provided_name: Optional[str] = None) -> str:
    """
    Get the collection name to use, with the following priority:
    1. User-provided name
    2. Name from settings
    3. Default fallback name
    """
    if provided_name:
        return provided_name
    
    try:
        if settings and settings.chromadb and settings.chromadb.default_collection_name:
            name = settings.chromadb.default_collection_name
            logger.info(f"Using collection name from settings: {name}")
            return name
    except Exception as e:
        logger.warning(f"Failed to get collection name from settings: {e}")
    
    logger.info(f"Using default collection name: {DEFAULT_COLLECTION_NAME}")
    return DEFAULT_COLLECTION_NAME


def get_language(provided_language: Optional[str] = None) -> str:
    """Get the programming language to use."""
    if provided_language:
        return provided_language
    
    # Try environment variable
    env_language = os.getenv('AUTOMATION_LANGUAGE')
    if env_language:
        logger.info(f"Using language from environment: {env_language}")
        return env_language
    
    logger.info(f"Using default language: {DEFAULT_LANGUAGE}")
    return DEFAULT_LANGUAGE


def get_framework(provided_framework: Optional[str] = None, language: str = DEFAULT_LANGUAGE) -> str:
    """Get the test framework to use."""
    if provided_framework:
        return provided_framework
    
    # Try environment variable
    env_framework = os.getenv('AUTOMATION_FRAMEWORK')
    if env_framework:
        logger.info(f"Using framework from environment: {env_framework}")
        return env_framework
    
    # Use first supported framework for the language
    frameworks = SUPPORTED_FRAMEWORKS.get(language, [DEFAULT_FRAMEWORK])
    framework = frameworks[0]
    logger.info(f"Using default framework for {language}: {framework}")
    return framework


def validate_framework(language: str, framework: str) -> bool:
    """Validate that the framework is supported for the language."""
    return framework in SUPPORTED_FRAMEWORKS.get(language, [])


def initialize_vector_store(collection_name: str) -> ChromaDBManager:
    """Initialize vector store using configuration settings."""
    try:
        # Use settings to get ChromaDB configuration
        persist_directory = None
        if settings and settings.chromadb:
            persist_directory = getattr(settings.chromadb, 'persist_directory', None)
        
        # Fallback to environment variable or default
        if not persist_directory:
            persist_directory = os.getenv('CHROMA_DB_PERSIST_DIRECTORY', './chroma_data')
        
        # Expand user path
        persist_directory = os.path.expanduser(persist_directory)
        
        logger.info(f"Initializing vector store at: {persist_directory}")
        
        # Initialize vector store with LLM manager
        llm_manager = LLMManager()  # Uses settings configuration
        vector_store = ChromaDBManager(llm_manager, persist_directory=persist_directory)
        
        # Test connectivity by listing collections
        try:
            collections = vector_store.list_collections()
            logger.info(f"Vector store ready with {len(collections)} collections")
            return vector_store
        except Exception as e:
            logger.warning(f"Vector store connectivity test failed: {e}")
            return vector_store
            
    except Exception as e:
        logger.error(f"Failed to initialize vector store: {e}")
        raise typer.Exit(code=1) from e


def automate_command(
    input_file: Annotated[str, typer.Argument(help="Path to test cases file (supports .md, .txt, .pdf, .docx, .xlsx)")],
    collection_name: Annotated[str, typer.Option(
        "--collection-name", "-c", help="ChromaDB collection name for application context")] = None,
    language: Annotated[str, typer.Option(
        "--language", "-l", help="Programming language for test automation")] = None,
    framework: Annotated[str, typer.Option(
        "--framework", "-F", help="Test framework to use")] = None,
    output_dir: Annotated[str, typer.Option(
        "--output-dir", "-o", help="Output directory for generated tests")] = DEFAULT_OUTPUT_DIR,
    interactive: Annotated[bool, typer.Option(
        "--interactive", "-i", help="Interactive mode to select test cases")] = False,
    num_context_docs: Annotated[int, typer.Option(
        "--num-context", "-n", min=1, max=20, help="Number of context documents to retrieve")] = 5,
    verbose: Annotated[bool, typer.Option(
        "--verbose", "-v", help="Enable verbose logging")] = False
):
    """Generate automation code using RAG-enhanced approach with vector store knowledge."""
    
    # Configure logging
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    print("ğŸš€ RAG-Enhanced Test Automation Generation")
    print("=" * 50)
    
    # Validate input file exists
    input_path = Path(input_file)
    if not input_path.exists():
        print(f"âŒ Error: Test cases file '{input_file}' not found.")
        raise typer.Exit(code=1)
    
    # Resolve configuration parameters
    collection_name = get_collection_name(collection_name)
    language = get_language(language)
    framework = get_framework(framework, language)
    
    # Validate framework compatibility
    if not validate_framework(language, framework):
        print(f"âŒ Error: Framework '{framework}' is not supported for language '{language}'.")
        print(f"Supported frameworks for {language}: {', '.join(SUPPORTED_FRAMEWORKS[language])}")
        raise typer.Exit(code=1)
    
    # Show resolved configuration
    print(f"âœ… Configuration:")
    print(f"   â€¢ Language: {language}")
    print(f"   â€¢ Framework: {framework}")
    print(f"   â€¢ Collection: {collection_name}")
    print(f"   â€¢ Output: {output_dir}")
    print(f"   â€¢ Context docs: {num_context_docs}")
    
    try:
        # 1. Initialize Vector Store
        def init_vector_store():
            return initialize_vector_store(collection_name)
        
        vector_store = with_progress_bar_sync(
            init_vector_store,
            "ğŸ” Initializing vector store and application context..."
        )
        
        # 2. Initialize LLM Manager from settings
        def init_llm():
            return LLMManager()  # Uses configuration from settings
        
        llm_manager = with_progress_bar_sync(
            init_llm,
            "ğŸ¤– Initializing LLM for code generation..."
        )
        
        print(f"âœ… RAG system ready with LLM provider: {llm_manager.provider}")
        
        # 3. Parse test cases using unified parser
        file_extension = input_path.suffix.lower()
        
        if file_extension not in ['.md', '.txt', '.pdf', '.docx', '.xlsx']:
            print(f"âŒ Unsupported file format: {file_extension}")
            print("Supported formats: .md, .txt, .pdf, .docx, .xlsx")
            raise typer.Exit(code=1)
        
        # Parse document for automation
        unified_parser = UnifiedDocumentParser()
        
        def parse_operation():
            return asyncio.run(unified_parser.parse_for_automation(input_path))
        
        parsed_doc = with_progress_bar_sync(
            parse_operation,
            f"ğŸ“– Parsing test cases from {input_path.name}..."
        )
        
        # Extract test cases
        test_cases = parsed_doc.test_cases
        
        # Fallback to markdown parser if needed
        if not test_cases and file_extension == '.md':
            def fallback_parse():
                md_parser = MarkdownTestCaseParser()
                return md_parser.parse_file(input_path)
            
            test_cases = with_progress_bar_sync(
                fallback_parse,
                "ğŸ”„ Using markdown-specific parser..."
            )
        
        if not test_cases:
            print("âŒ No test cases found in the file.")
            print("ğŸ’¡ Ensure the file contains structured test cases in the expected format.")
            raise typer.Exit(code=1)
        
        # Show parsing results
        print(f"\nâœ… Test cases parsed successfully!")
        print(f"   â€¢ Found: {len(test_cases)} test cases")
        if parsed_doc.metadata.title:
            print(f"   â€¢ Document: {parsed_doc.metadata.title}")
        print(f"   â€¢ Content: {parsed_doc.metadata.word_count} words")
        
        # Interactive selection if requested
        if interactive:
            selected_cases = interactive_select_tests(test_cases)
            if not selected_cases:
                print("âŒ No test cases selected.")
                raise typer.Exit(code=1)
            test_cases = selected_cases
            print(f"âœ… Selected {len(test_cases)} test cases for automation")
        
        # 4. Generate automation code using RAG approach
        output_path = Path(output_dir)
        
        # Create RAG-enhanced generator
        generator = RAGEnhancedTestGenerator(
            framework=framework,
            output_dir=output_path,
            vector_store=vector_store,
            language=language,
            llm_manager=llm_manager,
            num_context_docs=num_context_docs
        )
        
        def rag_generate_operation():
            return generator.generate(test_cases)
        
        generated_files = with_progress_bar_sync(
            rag_generate_operation,
            f"ğŸ§  Generating {language}/{framework} tests with application context..."
        )
        
        # 5. Write files
        def write_operation():
            return generator.write_files(generated_files)
        
        with_progress_bar_sync(
            write_operation,
            f"ğŸ“ Writing {len(generated_files)} files to {output_dir}..."
        )
        
        # 6. Success Summary
        print(f"\nğŸ‰ Test Generation Complete!")
        print(f"âœ… Generated {len(generated_files)} files using RAG-enhanced approach:")
        
        for file_name in sorted(generated_files.keys()):
            file_path = output_path / file_name
            file_size = len(generated_files[file_name])
            print(f"   â€¢ {file_name} ({file_size:,} chars)")
        
        print(f"\nğŸ“ Output directory: {output_path.absolute()}")
        
        # 7. Next steps
        print_next_steps(language, framework, output_path)
        
        # 8. Quality assessment
        assess_generated_quality(generated_files)
        
    except Exception as e:
        logger.error(f"Test generation failed: {e}", exc_info=True)
        print(f"\nâŒ Error: {e}")
        raise typer.Exit(code=1)


def interactive_select_tests(test_cases):
    """Interactive test case selection."""
    print("\nğŸ“‹ Available test cases:")
    for i, tc in enumerate(test_cases, 1):
        objective = tc.objective[:60] + "..." if len(tc.objective) > 60 else tc.objective
        print(f"{i:3d}. [{tc.id}] {objective}")
    
    print("\nSelect test cases to automate:")
    print("  â€¢ Enter numbers separated by commas (e.g., 1,3,5)")
    print("  â€¢ Enter ranges (e.g., 1-5)")
    print("  â€¢ Enter 'all' to select all tests")
    print("  â€¢ Enter 'none' to cancel")
    
    selection = typer.prompt("\nYour selection").strip().lower()
    
    if selection == 'none':
        return []
    elif selection == 'all':
        return test_cases
    else:
        selected_indices = parse_selection(selection, len(test_cases))
        return [test_cases[i-1] for i in selected_indices]


def parse_selection(selection: str, max_index: int) -> list:
    """Parse user selection string into list of indices."""
    indices = set()
    
    for part in selection.split(','):
        part = part.strip()
        if '-' in part:
            # Range
            try:
                start, end = map(int, part.split('-'))
                indices.update(range(max(1, start), min(max_index + 1, end + 1)))
            except ValueError:
                print(f"âš ï¸  Invalid range: {part}")
        else:
            # Single number
            try:
                num = int(part)
                if 1 <= num <= max_index:
                    indices.add(num)
                else:
                    print(f"âš ï¸  Index out of range: {num}")
            except ValueError:
                print(f"âš ï¸  Invalid number: {part}")
    
    return sorted(indices)


def print_next_steps(language: str, framework: str, output_dir: Path):
    """Print next steps for generated tests."""
    print("\nğŸ“š Next Steps:")
    
    if language == 'python':
        print(f"  1. cd {output_dir}")
        print("  2. pip install -r requirements.txt")
        if framework == 'pytest':
            print("  3. pytest --verbose")
            print("  4. pytest --html=report.html  # For HTML reports")
        elif framework == 'playwright':
            print("  3. playwright install  # Install browsers")
            print("  4. pytest --headed  # Run with visible browser")
        else:
            print("  3. python -m unittest discover -v")
    
    elif language in ('javascript', 'typescript'):
        print(f"  1. cd {output_dir}")
        print("  2. npm install")
        if framework == 'playwright':
            print("  3. npx playwright install")
            print("  4. npm test")
        else:
            print("  3. npm test")
    
    print("\nâœ¨ RAG-Enhanced Features:")
    print("  â€¢ Tests use real application endpoints and selectors")
    print("  â€¢ Authentication flows based on discovered patterns")
    print("  â€¢ Test data matches application schemas")
    print("  â€¢ Framework best practices applied automatically")


def assess_generated_quality(generated_files: dict):
    """Assess and report on the quality of generated tests."""
    total_lines = 0
    todo_count = 0
    test_function_count = 0
    
    for file_name, content in generated_files.items():
        if file_name.endswith(('.py', '.js', '.ts')):  # Test files
            lines = content.split('\n')
            total_lines += len(lines)
            
            # Count TODOs
            todo_count += sum(1 for line in lines if 'TODO' in line or 'FIXME' in line)
            
            # Count test functions
            test_function_count += sum(1 for line in lines 
                                     if any(pattern in line for pattern in 
                                           ['def test_', 'it(', 'test(', 'describe(']))
    
    print(f"\nğŸ“Š Quality Assessment:")
    print(f"   â€¢ Total Lines of Code: {total_lines:,}")
    print(f"   â€¢ Test Functions: {test_function_count}")
    print(f"   â€¢ TODO Items: {todo_count}")
    
    # Calculate quality score
    quality_score = max(0, 100 - (todo_count * 10)) if test_function_count > 0 else 0
    print(f"   â€¢ Quality Score: {quality_score}%")
    
    if quality_score >= 90:
        print("   ğŸŸ¢ Excellent: Tests should run with minimal modifications")
    elif quality_score >= 70:
        print("   ğŸŸ¡ Good: Some minor updates may be needed")
    elif quality_score >= 50:
        print("   ğŸŸ  Fair: Some manual work may be required")
    else:
        print("   ğŸ”´ Needs work: Significant manual implementation needed")